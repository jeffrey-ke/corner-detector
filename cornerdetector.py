# -*- coding: utf-8 -*-
"""CornerDetector.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ENxXCXJC7OsHIttuN9NcXALsNDUfsaJK
"""

import os
import argparse
from typing import Callable, Literal
import numpy as np
from tqdm import tqdm
import torch
import math
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms.functional as tvf
import torchvision.transforms.v2 as transforms
import cv2
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import torch.optim as optim
import shutil
import tempfile
from functools import partial

from .dataloader import CornerSet, ImgCorners


def prepare_corner_model(device="cuda", path_to_ckpt="CornerDetector/best.pt"):
    corner_model = SimpleCNN(8, (224, 224)).to(device)
    corner_model.load_state_dict(torch.load(path_to_ckpt, map_location=device)['model_state_dict'])
    denorm = partial(denormalize_corners, height=1080, width=1920)
    corner_model = TransformOutputs(
            corner_model,
            denorm
    )
    return corner_model

class TransformOutputs(nn.Module):
    def __init__(self, model: nn.Module, transforms: transforms.Compose | Callable[[torch.Tensor], torch.Tensor]):
        super().__init__()
        self.model = model
        self.transforms = transforms

    def forward(self, x):
        return self.transforms(self.model(x))

class TransformInputs(nn.Module):
    def __init__(self, model: nn.Module, transforms: transforms.Compose | Callable[[torch.Tensor], torch.Tensor]):
        super().__init__()
        self.model = model
        self.transforms = transforms

    def forward(self, x):
        return self.model(self.transforms(x))

class SimpleCNN(nn.Module):
    def __init__(self, numcorners, input_size, c_dim=3):
        super().__init__()
        self.input_size = input_size

        self.conv1 = nn.Conv2d(c_dim, 6, (5,5))
        self.conv2 = nn.Conv2d(6, 16, (5,5))
        self.pool = nn.AvgPool2d(2,2)

        self.flat_dim = (input_size[0] // 4 - 3) * (input_size[1] // 4 - 3) * 16
        self.fc1 = nn.Linear(self.flat_dim, 120)
        self.fc2 = nn.Linear(120, 64)
        self.fc3 = nn.Linear(64, numcorners)

    def get_transforms(self, train_or_eval: Literal["train", "eval"]):
        assert train_or_eval in ["train", "eval"]
        trainforms = transforms.Compose([
                transforms.ToDtype(torch.float32, scale=True),
                transforms.Resize(self.input_size, antialias=True),
                transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ])
        evalforms = transforms.Compose([
                transforms.ToDtype(torch.float32, scale=True),
                transforms.Resize(self.input_size, antialias=True),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
        return trainforms if train_or_eval == 'train' else evalforms

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.reshape(-1, self.flat_dim)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        out = self.fc3(x)
        return out.view(-1, 4, 2)


class CornerNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.body = nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(),
            nn.Conv2d(32, 32, 3, padding=1), nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),
            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(64,128,3,padding=1), nn.ReLU(),
            nn.Conv2d(128,128,3,padding=1), nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(128,256,3,padding=1), nn.ReLU(),
            nn.AdaptiveAvgPool2d(1),
        )
        self.fc = nn.Sequential(
            nn.Flatten(),
            nn.Dropout(0.3),
            nn.Linear(256, 128), nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 8)   # 4*2
        )
    
    def forward(self,x):
        x = self.body(x)
        x = self.fc(x)
        return x.view(-1,4,2)


def normalize_corners(corners: torch.Tensor, width: int, height: int) -> torch.Tensor:
    # corners: (..., 4, 2) in pixels
    c = corners.clone()
    c[..., 0] = c[..., 0] / max(1, width)
    c[..., 1] = c[..., 1] / max(1, height)
    return c

def denormalize_corners(corners: torch.Tensor, width: int, height: int) -> torch.Tensor:
    # corners: (..., 4, 2) in [0,1]
    c = corners.clone()
    c[..., 0] = c[..., 0] * width
    c[..., 1] = c[..., 1] * height
    return c


def denormalize_px(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):
    """Denormalized tensor in [0, 1] range"""
    mean = torch.tensor(mean).view(3, 1, 1).to(tensor)
    std = torch.tensor(std).view(3, 1, 1).to(tensor)
    return (tensor * std + mean).clamp(0, 1)


def train_val_split(dataset, args, seed):
    val_ratio = args.val_ratio
    batch_size = args.batch_size

    assert 0 < val_ratio < 1, f"val_ratio must be between 0 and 1, got {val_ratio}"

    torch.manual_seed(seed)

    train_dataset, val_dataset = torch.utils.data.random_split(
        dataset,
        [1 - val_ratio, val_ratio],
        generator=torch.Generator().manual_seed(seed)
    )

    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        collate_fn=ImgCorners.collate,
        batch_size=batch_size,
        shuffle=True,
    )

    val_loader = torch.utils.data.DataLoader(
        val_dataset,
        collate_fn=ImgCorners.collate,
        batch_size=1,
        shuffle=False,
    )

    return train_loader, val_loader, train_dataset, val_dataset


def visualize_validation(imgs, gt_corners, pred_corners):
    assert imgs.shape[0] == 1, "Batch size must be one for the valloader!"
    imgs = imgs.cpu().squeeze()
    gt_corners = gt_corners.cpu().squeeze()
    pred_corners = pred_corners.cpu().squeeze()

    # imgs expected uint8 CHW RGB
    img_pil = cv2.cvtColor(np.array(tvf.to_pil_image(imgs)), cv2.COLOR_RGB2BGR)

    # draw GT (green) and preds (red)
    for (gt_x, gt_y), (pred_x, pred_y) in zip(gt_corners, pred_corners):
        gt_x, gt_y = int(round(float(gt_x))), int(round(float(gt_y)))
        pred_x, pred_y = int(round(float(pred_x))), int(round(float(pred_y)))
        cv2.circle(img_pil, (gt_x, gt_y), 3, (0, 255, 0), -1)
        cv2.circle(img_pil, (pred_x, pred_y), 3, (0, 0, 255), -1)
    return img_pil


def train(args):
    """Training function"""
    # Setup transforms
    transform_train = transforms.Compose([
        transforms.ToDtype(torch.float32, scale=True),
        transforms.Resize(args.input_size, antialias=True),
        transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])
    transform_test = transforms.Compose([
        transforms.ToDtype(torch.float32, scale=True),
        transforms.Resize(args.input_size, antialias=True),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # Load dataset
    cornerset = CornerSet(args.data_dir)
    trainloader, valloader, _, _ = train_val_split(cornerset, args, args.seed)

    # Setup model
    if args.model == "simple":
        model = SimpleCNN(8, args.input_size)
    else:
        model = CornerNet()
    
    loss_fn = nn.SmoothL1Loss(beta=0.5)
    optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    # Load checkpoint if resuming
    start_epoch = 0
    best_val = float('inf')
    wait = 0
    
    if args.resume:
        if os.path.exists(args.resume):
            print(f"Loading checkpoint from {args.resume}")
            checkpoint = torch.load(args.resume, map_location=device)
            if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                model.load_state_dict(checkpoint['model_state_dict'])
                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                start_epoch = checkpoint.get('epoch', 0)
                best_val = checkpoint.get('best_val', float('inf'))
            else:
                model.load_state_dict(checkpoint)
            print(f"Resumed from epoch {start_epoch}")
        else:
            print(f"Warning: Resume checkpoint {args.resume} not found, starting from scratch")

    # Training loop
    model.train()
    for epoch in range(start_epoch, args.epochs):
        epoch_train_loss = 0.0
        epoch_train_px_mae = 0.0
        num_train_batches = 0

        pbar = tqdm(enumerate(trainloader), desc=f"Epoch {epoch+1}/{args.epochs}")
        for batch_idx, batch in pbar:
            imgs = batch.img                      # uint8 B,C,H,W (original)
            corners = batch.corner                # B,4,2 in pixels (original)

            orig_h, orig_w = imgs.shape[-2], imgs.shape[-1]
            # Normalize targets to [0,1] w.r.t original dims (consistent with pure resize)
            corners_norm = normalize_corners(corners, orig_w, orig_h)

            imgs = transform_train(imgs.to(device))
            corners_norm = corners_norm.to(device)

            pred = model(imgs)                    # B,4,2 (unbounded)
            if args.use_sigmoid:
                pred = torch.sigmoid(pred)        # bound to [0,1]

            loss = loss_fn(pred, corners_norm)

            optimizer.zero_grad()
            loss.backward()
            if args.grad_clip > 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)
            optimizer.step()

            # Diagnostics in pixel space (on CPU)
            with torch.no_grad():
                pred_px = denormalize_corners(pred.detach().cpu(), orig_w, orig_h)
                px_mae = (pred_px - corners).abs().mean().item()

            epoch_train_loss += loss.item()
            epoch_train_px_mae += px_mae
            num_train_batches += 1
            pbar.set_postfix(loss=f"{loss.item():.4f}", px_mae=f"{px_mae:.2f}px")

        epoch_train_loss /= max(1, num_train_batches)
        epoch_train_px_mae /= max(1, num_train_batches)

        # Validation
        model.eval()
        val_loss_total = 0.0
        val_px_mae_total = 0.0
        num_val_batches = 0

        with torch.no_grad():
            for batch_idx, batch in enumerate(valloader):
                imgs = batch.img                  # uint8 B=1
                gt_corners = batch.corner         # B,4,2 pixels

                orig_h, orig_w = imgs.shape[-2], imgs.shape[-1]
                gt_norm = normalize_corners(gt_corners, orig_w, orig_h)

                imgs_t = transform_test(imgs.to(device))
                gt_norm = gt_norm.to(device)

                pred_val = model(imgs_t)
                if args.use_sigmoid:
                    pred_val = torch.sigmoid(pred_val)

                batch_val_loss = loss_fn(pred_val, gt_norm)
                val_loss_total += batch_val_loss.item()
                num_val_batches += 1

                # Pixel MAE for validation
                pred_px = denormalize_corners(pred_val.detach().cpu(), orig_w, orig_h)
                val_px_mae_total += (pred_px - gt_corners).abs().mean().item()

        epoch_val_loss = val_loss_total / max(1, num_val_batches)
        epoch_val_px_mae = val_px_mae_total / max(1, num_val_batches)
        print(f"Epoch {epoch+1}/{args.epochs} | Train Loss: {epoch_train_loss:.6f} "
              f"| Val Loss: {epoch_val_loss:.6f} | Train px-MAE: {epoch_train_px_mae:.2f} "
              f"| Val px-MAE: {epoch_val_px_mae:.2f}")

        # Early stopping and checkpointing
        if epoch_val_loss < best_val:
            best_val = epoch_val_loss
            wait = 0

            checkpoint = {
                'epoch': epoch + 1,
                'model_state_dict': {k: v.cpu() for k, v in model.state_dict().items()},
                'optimizer_state_dict': optimizer.state_dict(),
                'best_val': best_val,
                'args': vars(args)
            }

            save_dir = os.path.dirname(args.save_path) or "."
            fd, tmp_file = tempfile.mkstemp(dir=save_dir, prefix="tmp_best_", suffix=".pt")
            os.close(fd)
            try:
                torch.save(checkpoint, tmp_file)
                os.replace(tmp_file, args.save_path)
                print(f"Saved best checkpoint to {args.save_path}")
            except Exception as e:
                print(f"Warning: Failed to save checkpoint: {e}")
                shutil.move(tmp_file, args.save_path)
        else:
            wait += 1

        if wait >= args.patience:
            print("Early stopping triggered.")
            break

        model.train()
        scheduler.step()


def evaluate(args):
    """Evaluation function"""
    transform_test = transforms.Compose([
        transforms.ToDtype(torch.float32, scale=True),
        transforms.Resize(args.input_size, antialias=True),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # Load dataset
    cornerset = CornerSet(args.data_dir)
    valloader = DataLoader(cornerset, batch_size=1, 
                          collate_fn=ImgCorners.collate, shuffle=False)

    # Setup model
    if args.model == "simple":
        model = SimpleCNN(8, args.input_size)
    else:
        model = CornerNet()
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # Load checkpoint
    if not os.path.exists(args.checkpoint):
        raise FileNotFoundError(f"Checkpoint not found: {args.checkpoint}")
    
    print(f"Loading checkpoint from {args.checkpoint}")
    checkpoint = torch.load(args.checkpoint, map_location=device)
    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
        print(f"Loaded model from epoch {checkpoint.get('epoch', 'unknown')}")
    else:
        model.load_state_dict(checkpoint)
    
    model = model.to(device)
    model.eval()

    os.makedirs(args.output_dir, exist_ok=True)
    loss_fn = nn.SmoothL1Loss(beta=0.5)
    total_loss = 0.0
    total_px_mae = 0.0
    num_samples = 0

    print("Running evaluation...")
    with torch.no_grad():
        for i, batch in enumerate(tqdm(valloader)):
            imgs = batch.img
            gt_corners = batch.corner

            orig_h, orig_w = imgs.shape[-2], imgs.shape[-1]
            gt_norm = normalize_corners(gt_corners, orig_w, orig_h)

            imgs_t = transform_test(imgs.to(device))
            gt_norm = gt_norm.to(device)

            output = model(imgs_t)
            if args.use_sigmoid:
                output = torch.sigmoid(output)

            loss = loss_fn(output, gt_norm)
            total_loss += loss.item()

            pred_on_orig = denormalize_corners(output.detach().cpu(), orig_w, orig_h)
            total_px_mae += (pred_on_orig - gt_corners).abs().mean().item()
            num_samples += 1

            vis = visualize_validation(imgs, gt_corners, pred_on_orig)
            cv2.imwrite(f"{args.output_dir}/eval_{i:04d}.png", vis)

    avg_loss = total_loss / max(1, num_samples)
    avg_px_mae = total_px_mae / max(1, num_samples)
    print(f"Evaluation complete. Average Loss: {avg_loss:.6f} | Avg px-MAE: {avg_px_mae:.2f}")
    print(f"Results saved to {args.output_dir}")


def main():
    parser = argparse.ArgumentParser(description="Corner Detector Training and Evaluation")
    
    # Mode
    parser.add_argument('--mode', type=str, choices=['train', 'eval'], required=True,
                       help='Mode: train or eval')
    
    # Data
    parser.add_argument('--data-dir', type=str, 
                       default="render_0",
                       help='Path to dataset directory')
    parser.add_argument('--output-dir', type=str, default='val',
                       help='Output directory for evaluation results')
    
    # Model
    parser.add_argument('--model', type=str, choices=['simple', 'cornernet'], 
                       default='simple', help='Model architecture')
    parser.add_argument('--input-size', type=int, nargs=2, default=[224, 224],
                       help='Input image size (H W)')
    
    # Training hyperparameters
    parser.add_argument('--epochs', type=int, default=50, help='Number of epochs')
    parser.add_argument('--batch-size', type=int, default=16, help='Batch size')
    parser.add_argument('--val-ratio', type=float, default=0.2, help='Validation split ratio')
    parser.add_argument('--seed', type=int, default=42, help='Random seed for reproducibility')
    parser.add_argument('--lr', type=float, default=1e-3, help='Learning rate')
    parser.add_argument('--weight-decay', type=float, default=1e-4, 
                       help='Weight decay')
    parser.add_argument('--grad-clip', type=float, default=0.0,
                       help='Gradient clipping threshold (0 to disable)')
    parser.add_argument('--patience', type=int, default=10,
                       help='Early stopping patience')
    
    # Checkpointing
    parser.add_argument('--save-path', type=str, default='best.pt',
                       help='Path to save best model checkpoint')
    parser.add_argument('--resume', type=str, default=None,
                       help='Path to checkpoint to resume training from')
    parser.add_argument('--checkpoint', type=str, default='best.pt',
                       help='Path to checkpoint for evaluation')
    parser.add_argument('--use-sigmoid', action='store_true', default=False,
                        help='Bound predictions to [0,1] with sigmoid')
    
    args = parser.parse_args()
    args.input_size = tuple(args.input_size)
    
    if args.mode == 'train':
        train(args)
    elif args.mode == 'eval':
        evaluate(args)


if __name__ == "__main__":
    main()
